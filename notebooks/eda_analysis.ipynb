{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime\n",
    "\n",
    "\n",
    "news_df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "news_df.head()\n",
    "news_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['headline_length'] = news_df['headline'].apply(len)\n",
    "print(news_df['headline_length'].describe())\n",
    "\n",
    "# Count articles per publisher\n",
    "publisher_counts = news_df['publisher'].value_counts()\n",
    "publisher_counts.plot(kind='bar', title='Articles per Publisher')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "news_df.set_index('date')['headline'].resample('D').count().plot(figsize=(12,5), title='Articles Published Over Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf11f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top publishers\n",
    "top_publishers = news_df['publisher'].value_counts().head(10)\n",
    "\n",
    "# Unique email domains (if applicable)\n",
    "if news_df['publisher'].str.contains('@').any():\n",
    "    news_df['domain'] = news_df['publisher'].str.extract(r'@([\\w.]+)')\n",
    "    print(news_df['domain'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(news_df['headline'].fillna(\"\"))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display topics\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"TOP 10 WORDS FOR TOPIC #{index}\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
